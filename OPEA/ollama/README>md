OPEA Comps

## Running Ollama Third party

### setting values

- checking documentation https://hub.docker.com/r/ollama/ollama
- checking opea.dev for ollama

  ask claude.ai asking what values I need to set

services:
ollama-server:
image: ollama/ollama
container_name: ollama-server
ports: - ${LLM_ENDPOINT_PORT:-8008}:11434
environment:
no_proxy: ${no_proxy}
http_proxy: ${http_proxy}
https_proxy: ${https_proxy}
LLM_MODEL_ID: ${LLM_MODEL_ID}
host_ip: ${host_ip}

networks:
default:
driver: bridge

## Homework

Difficulty: Level 200

Business Goal:
The company wants you to explore the effort it would take to run the AI workloads completely on servers that will live in-house. The fractional CTO, suggests that its best practice to run workloads in containers or kubenetes. You as the AI Engineer have been tasked to determine how to learn to work with the building blocks to constructor your own GenAI workloads running on containers.

Technical Uncertainty
Using OPEA does it serve the model via a LLM server?
How do we orchestrate two services together?
What is the quality of build across the various OPEA Comps?

Technical Restrictions
GenAIComps [GitHub Repo](https://github.com/opea-project/GenAIComps)
OPEA Comps Project [Readme](https://opea-project.github.io/latest/GenAIComps/README.html)
Docker Containers // [opea + third party + ollama](https://github.com/opea-project/GenAIComps/blob/main/comps/third_parties/ollama/deployment/docker_compose/compose.yaml)

Homework Challenges
Orchestrate multiple services eg. 2 or 3 together
Or Try and get a different comp working that Andrew did use not use.

Homework Bonuses
Make a tutorial or technical doc that is public on LinkedIn, Medium, Hashnode, Your Blog
Tag Andrew or show off the work in the Discord show-and-tell.

## Andrew Notes

## Running Ollama Third-Party Service

### Choosing a Model

You can get the model_id that ollama will launch from the [Ollama Library](https://ollama.com/library).

https://ollama.com/library/llama3.2

eg. LLM_MODEL_ID="llama3.2:1b"

### Getting the Host IP

#### Linux

Get your IP address

```sh
sudo apt install net-tools
ifconfig
```

```
LLM_ENDPOINT_PORT=8008 LLM_MODEL_ID="llama3.2:1b"
host_ip=172.24.230.22 docker-compose up
```

Or you can try this way `$(hostname -I | awk '{print $1}')`

HOST_IP=$(hostname -I | awk '{print $1}') NO_PROXY=localhost LLM_ENDPOINT_PORT=9000 LLM_MODEL_ID="llama3.2:1b" docker compose up

### Ollama API

Once the Ollama server is running we can make API calls to the ollama API

https://github.com/ollama/ollama/blob/main/docs/api.md

## Download (Pull) a model

curl http://localhost:8008/api/pull -d '{
"model": "llama3.2:1b"
}'

## Generate a Request

curl http://localhost:/api/generate -d '{
"model": "llama3.2:1b",
"prompt": "Why is the sky blue?"
}'

# Technical Uncertainty

Q Does bridge mode mean we can only accses Ollama API with another model in the docker compose?

A No, the host machine will be able to access it

Q: Which port is being mapped 8008->141414

In this case 8008 is the port that host machine will access. the other other in the guest port (the port of the service inside container)

Q: If we pass the LLM_MODEL_Id to the ollama server will it download the model when on start?

It does not appear so. The ollama CLI might be running multiple APIs so you need to call the /pull api before trying to generat text

Q: Will the model be downloaded in the container? does that mean the ml model will be deleted when the container stops running?

A: The model will download into the container, and vanish when the container stop running. You need to mount a local drive and there is probably more work to be done.

Q: For LLM service which can text-generation it suggets it will only work with TGI/vLLM and all you have to do is have it running. Does TGI and vLLM have a stardarized API or is there code to detect which one is running? Do we have to really use Xeon or Guadi processor?

vLLM, TGI (Text Generation Inference), and Ollama all offer APIs with OpenAI compatibility, so in theory they should be interchangable.
